# Arbitrary unique name for the connector. Attempting to register
# two connectors with the same name will fail.
name=test-couchbase-sink

# The Java class for the connector.
connector.class=com.couchbase.connect.kafka.CouchbaseSinkConnector

# The maximum number of tasks that should be created for this connector.
tasks.max=2

# Read from these Kafka topics (comma-separated list).
topics=sink-topic2

# Connect to this Couchbase cluster (comma-separated list of bootstrap nodes).
#couchbase.seed.nodes=127.0.0.1
#couchbase.bootstrap.timeout=10s

# Optionally connect to Couchbase Server over a secure channel.
# If the KAFKA_COUCHBASE_TRUST_STORE_PASSWORD environment variable is set,
# it will override the password specified here.
#   couchbase.enable.tls=true
#   couchbase.trust.store.path=/path/to/keystore
#   couchbase.trust.store.password=secret


connection.cluster_address=127.0.0.1
connection.bucket=kafkaSinkBucket
connection.username=admin
connection.password=123456


# Write to this Couchbase bucket using these credentials.
# If the KAFKA_COUCHBASE_PASSWORD environment variable is set,
# it will override the password specified here.
#couchbase.bucket=kafkaSinkBucket
#couchbase.username=admin
#couchbase.password=123456

# Optionally set the Couchbase document ID from fields of the message body.
# The value is a format string with a placeholder for each field to include
# in the ID. A placeholder looks like "${/path/to/field}" where the path is
# specified as a JSON pointer. Examples:
#   couchbase.document.id=${/id}
#   couchbase.document.id=invoice::${/id}
#   couchbase.document.id=${/metadata/type}::${/id}

#couchbase.document.id=/

# Optionally remove the ID fields from the document before saving to Couchbase.
  #couchbase.remove.document.id=true
 # connection.remove.document.id=true
  #couchbase.remove.document.id=true

  #custom implementation of remove.document.id
  #aci.remove.document.id=com.aci.document.remove.RemoveDocumnetID
  
 # connection.remove.document.id=com.aci.document.remove.RemoveDocumnetID
  couchbase.remove.document.id=true
  
# Optionally specify Couchbase persistence requirements for a write to be
# considered successful. Default is NONE. Other values enhance durability
# but reduce performance. If the requested requirements cannot be met
# (due to Couchbase rebalance or failover, for instance) the connector will
# terminate. Possible values:
#  NONE   - Do not require any disk persistence.
#  MASTER - Require disk persistence to the master node of the document only.
#  ONE    - Require disk persistence of one node (master or replica).
#  TWO    - Require disk persistence of two nodes (master or replica).
#  THREE  - Require disk persistence of three nodes (master or replica).
#  FOUR   - Require disk persistence of four nodes (master + three replicas).
couchbase.persist.to=NONE

# Optionally specify Couchbase replication requirements for a write to be
# considered successful. Default is NONE. Other values enhance durability
# but reduce performance. If the requested requirements cannot be met
# (due to Couchbase rebalance or failover, for instance) the connector will
# terminate. Possible values:
#   NONE   - Do not require any replication.
#   ONE    - Require replication to one replica.
#   TWO    - Require replication to two replicas.
#   THREE  - Require replication to three replicas.
couchbase.replicate.to=NONE

# The following *.converter properties are appropriate when reading from
# a topic whose messages have String (or null) keys and raw JSON values,
# without schemas. These are the correct settings to use with the code in
# the examples/json-producer directory.
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter.schemas.enable=false

couchbase.n1ql.clause.fields=documentType:address
#couchbase.n1ql.clause=WHERE
##couchbase.n1ql.operation=UPDATE
couchbase.document.mode=N1QL
couchbase.subdocument.create_document=false
couchbase.create_document=true

#transforms= MakeMap,InsertSource
#transforms.MakeMap.type=org.apache.kafka.connect.transforms.HoistField$Value
#transforms.MakeMap.field=line
#transforms.InsertSource.type=org.apache.kafka.connect.transforms.InsertField$Value
#transforms.InsertSource.static.field=sink_id
#transforms.InsertSource.static.value=28July2020


#transforms= MakeMap,InsertSource
#transforms.MakeMap.type=org.apache.kafka.connect.transforms.HoistField$Value
#transforms.MakeMap.field=line
#transforms.InsertSource.type=org.apache.kafka.connect.transforms.InsertField$Value
#transforms.InsertSource.static.field=id
#transforms.InsertSource.static.value=05Aug-Jul2020

#transforms=tsRouter,insertKafkaCoordinates
 
#transforms.tsRouter.type=org.apache.kafka.connect.transforms.TimestampRouter
#transforms.tsRouter.topic.format=${topic}-${timestamp}
#transforms.tsRouter.timestamp.format=yyyyMMdd
  
#transforms.insertKafkaCoordinates.type=org.apache.kafka.connect.transforms.InsertInValue
##transforms.insertKafkaCoordinates.topic=kafkaSinkBucket
#transforms.insertKafkaCoordinates.partition=1
#transforms.insertKafkaCoordinates.offset=0

#transforms=reverse,lowercase

#transforms.reverse.type=com.couchbase.connect.kafka.example.CustomTransform
#transforms.reverse.op=REVERSE

##transforms.lowercase.type=com.couchbase.connect.kafka.example.CustomTransform
#transforms.lowercase.op=LOWER_CASE


transforms=removeDocumentID
transforms.removeDocumentID.type=com.couchbase.connect.kafka.deleteOldDocumentID.RemoveDocumentIDTransform

# If you're using Confluent, the Couchbase Sink Connector can also read
# messages published in the Avro format. Replace the above *.converter
# properties with the following (modifying the schema registry URL if needed):
#   key.converter=io.confluent.connect.avro.AvroConverter
#   key.converter.schema.registry.url=http://localhost:8081
#   value.converter=io.confluent.connect.avro.AvroConverter
#   value.converter.schema.registry.url=http://localhost:8081

# Optionally specify a time-to-live for documents written to Couchbase.
# If present, the value must be an integer followed by a time unit:
# (s = seconds, m = minutes, h = hours, d = days).
#   couchbase.document.expiration=30m

# If you're experimenting with the custom Single Message Transform from the
# example project, uncomment these lines:
#transforms=reverse,lowercase
#transforms.reverse.type=com.couchbase.connect.kafka.example.CustomTransform
#transforms.reverse.op=REVERSE
#transforms.lowercase.type=com.couchbase.connect.kafka.example.CustomTransform
#transforms.lowercase.op=LOWER_CASE
